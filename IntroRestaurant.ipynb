{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Restaurant atmosphere/vibes analysis"
      ],
      "metadata": {
        "id": "c_jtzDcMdCsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "gitCDheodFTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found some apps, that contain images of restaurants along with a vibe labelled.\n",
        "\n",
        "`Tripadvisor` only has `business meetings`, `romantic`, and `ideal for groups` vibes. So it is not enough.\n",
        "\n",
        "After searching a looot of apps, I found the `One Zone` app. Desgined specifically to chose a restaurant by the vibe they evoke. However, the dataset is really small, as they add restaurants manually. They don't use any kind of deep learning method. Maybe we can use it as a test dataset."
      ],
      "metadata": {
        "id": "HN-VANYpdHuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, what I am going to try to do, is to develop a NLP model to classify customer reviews by the restaurant vibe. Then I would label the restaurants images by the vibe obtained with the NLP model."
      ],
      "metadata": {
        "id": "HpZT3-7pdoth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(PONER IMAGEN DEL WORKFLOW QUE VOY A SEGUIR)**"
      ],
      "metadata": {
        "id": "dThXZOtqrXBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yelp Dataset"
      ],
      "metadata": {
        "id": "vtnQblvHrbaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we'll be working with Yelp dataset. The Yelp Open Dataset is an extensive collection provided by Yelp for academic and research purposes. It offers a rich source of data from Yelp's vast repository of local business reviews and user interactions. It contains:\n",
        "\n",
        "1. **Businesses:**\n",
        "Basic information about local businesses, including their name, location (address, city, state, postal code), latitude and longitude, average rating, category (e.g., Restaurants, Shopping, Beauty & Spas), and the number of reviews they have received. This allows for analyses focused on geographical trends, category-based studies, and more.\n",
        "\n",
        "2. **Reviews:**\n",
        "Millions of text reviews written by Yelp users for various businesses. Each review includes the user's text review, the star rating they gave, and the date of the review. This data is invaluable for sentiment analysis, natural language processing tasks, and understanding consumer preferences.\n",
        "\n",
        "3. **Users:**\n",
        "Information about Yelp users who have written reviews, including their user ID, name, review count, yelping since (the date they joined Yelp), friends, and other social metrics. This can be used for social network analysis, studying user behavior, and personalization algorithms.\n",
        "\n",
        "4. **Check-ins:**\n",
        "Data about check-ins at businesses by Yelp users, which can help in analyzing foot traffic trends, popular times, and loyalty or frequency of visits.\n",
        "\n",
        "5. **Tips:**\n",
        "Short notes left by users about a business. Tips can contain advice, recommendations, or comments about what's good or what to avoid. This dataset is useful for extracting quick insights or highlights about businesses.\n",
        "\n",
        "6. **Photos:**\n",
        "Metadata about photos associated with businesses, including a photo ID, business ID, caption, and labels indicating the photo category (e.g., food, interior, exterior).\n",
        "This can support visual analysis of businesses or complement review-based insights as in our case."
      ],
      "metadata": {
        "id": "6q6fXOs3riVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Yelp data"
      ],
      "metadata": {
        "id": "5_RCYv38sN5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exaEa8IV1aTg",
        "outputId": "9b6ca836-9860-45a5-a016-60c7975e0b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/yelp.zip\" -d \"/content/data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ULE4JhU1hGI",
        "outputId": "8996cd8e-855c-42ae-83d3-79732d37b0be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/yelp.zip\n",
            "  inflating: /content/data/Dataset_User_Agreement.pdf  \n",
            "  inflating: /content/data/yelp_academic_dataset_business.json  \n",
            "  inflating: /content/data/yelp_academic_dataset_checkin.json  \n",
            "  inflating: /content/data/yelp_academic_dataset_review.json  \n",
            "  inflating: /content/data/yelp_academic_dataset_tip.json  \n",
            "  inflating: /content/data/yelp_academic_dataset_user.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "QUFCHtWw1-bu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore the data a bit. We just want to analyse restaurants."
      ],
      "metadata": {
        "id": "GPnRNoFSsPo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "json_file_path = '/content/data/yelp_academic_dataset_review.json'  # Update this to your file path\n",
        "\n",
        "# Define a list to hold chunks of the DataFrame\n",
        "df_list = []\n",
        "\n",
        "# Use a chunk size that fits well into your memory. Adjust as necessary.\n",
        "chunksize = 10 ** 5\n",
        "\n",
        "# Read the JSON file in chunks\n",
        "with pd.read_json(json_file_path, lines=True, chunksize=chunksize) as reader:\n",
        "    for chunk in reader:\n",
        "        # Filter out only the 'business_id' and 'text' columns\n",
        "        filtered_chunk = chunk[['business_id', 'text']]\n",
        "        df_list.append(filtered_chunk)\n",
        "\n",
        "# Concatenate all chunks into a single DataFrame\n",
        "df = pd.concat(df_list, ignore_index=True)"
      ],
      "metadata": {
        "id": "JT4rPKKg4d7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/content/drive/My Drive/yelp_reviews.csv'  # Specify your desired path\n",
        "df.to_csv(csv_file_path, index=False)"
      ],
      "metadata": {
        "id": "B76vo3Bc5YXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nnV5gpn5riX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_merged = pd.read_csv(\"/content/drive/MyDrive/yelp_reviews.csv\")"
      ],
      "metadata": {
        "id": "300fFxHJ8IHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Path to your business JSON file\n",
        "business_json_path = '/content/data/yelp_academic_dataset_business.json'\n",
        "\n",
        "# Load the business data\n",
        "business_df = pd.read_json(business_json_path, lines=True)\n",
        "\n",
        "# Filter for restaurants\n",
        "# Note: Adjust the condition as necessary to accurately capture all restaurants in your dataset\n",
        "restaurant_df = business_df[business_df['categories'].str.contains('Restaurants', case=False, na=False)]"
      ],
      "metadata": {
        "id": "IuwuYUPc8M49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "39ZHn7RQriac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.merge(data_merged, restaurant_df[['business_id', 'name', 'longitude', 'latitude', 'review_count']], on='business_id', how='left')"
      ],
      "metadata": {
        "id": "s3qS8r0p-0wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/content/drive/My Drive/yelp_restaurants.csv'  # Specify your desired path\n",
        "final_df.to_csv(csv_file_path, index=False)"
      ],
      "metadata": {
        "id": "kplfWsIe9bRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_df = final_df.sort_values(by='business_id', ascending=True)"
      ],
      "metadata": {
        "id": "1Q-gNwYXAvCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring some NLP models"
      ],
      "metadata": {
        "id": "kvtw4mGvdy1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RoBERTa-Large Variant"
      ],
      "metadata": {
        "id": "frbqQ1Ayd0wF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore this model."
      ],
      "metadata": {
        "id": "TYgvqnQfeuON"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "wzmzLKBXc-s7",
        "outputId": "f566a6ea-f581-4523-cfdc-14e94343d2fb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-40977de58fb8>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2009\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer."
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-classification\", model=\"AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gNhNjQmHihWA",
        "outputId": "5e58d26b-46bc-416d-9b2a-c952fc2f5ba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Could not load model AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>, <class 'transformers.models.roberta.modeling_tf_roberta.TFRobertaForMaskedLM'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3098, in from_pretrained\n    raise EnvironmentError(\nOSError: AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 2829, in from_pretrained\n    raise EnvironmentError(\nOSError: AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain does not appear to have a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\nwhile loading with RobertaForMaskedLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3098, in from_pretrained\n    raise EnvironmentError(\nOSError: AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFRobertaForMaskedLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 2829, in from_pretrained\n    raise EnvironmentError(\nOSError: AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain does not appear to have a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\n\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a7818a577e05>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text-classification\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mmodel_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargeted_task\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         framework, model = infer_framework_load_model(\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mmodel_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_traceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"while loading with {class_name}, an error is thrown:\\n{trace}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    283\u001b[0m                 \u001b[0;34mf\"Could not load model {model} with any of the following classes: {class_tuple}. See the original errors:\\n\\n{error}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Could not load model AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForMaskedLM'>, <class 'transformers.models.roberta.modeling_tf_roberta.TFRobertaForMaskedLM'>). See the original errors:\n\nwhile loading with AutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3098, in from_pretrained\n    raise EnvironmentError(\nOSError: AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFAutoModelForSequenceClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 2829, in from_pretrained\n    raise EnvironmentError(\nOSError: AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain does not appear to have a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\nwhile loading with RobertaForMaskedLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 3098, in from_pretrained\n    raise EnvironmentError(\nOSError: AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\nwhile loading with TFRobertaForMaskedLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 2829, in from_pretrained\n    raise EnvironmentError(\nOSError: AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain does not appear to have a file named pytorch_model.bin, tf_model.h5 or model.ckpt\n\n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_id = \"AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "l73n5-1_fxq_",
        "outputId": "8c8b45f4-7677-48eb-cb49-378c488af1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8ce95651f492>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2009\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'AliHaider0343/implicit-and-explicit-aspects-Extraction-in-Restaurant-Reviews-Domain' is the correct path to a directory containing all relevant files for a RobertaTokenizerFast tokenizer."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LkRn-9x3fTyG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}